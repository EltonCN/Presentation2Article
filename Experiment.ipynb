{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #Operações com o SO (arquivos)\n",
    "import json #Leitura/escrita de arquivos JSON\n",
    "import time #Sleep\n",
    "import threading #Multithreading\n",
    "import abc #Classes abstratas\n",
    "from typing import Optional, Dict, List, Tuple, Any #Type hints\n",
    "\n",
    "import tqdm #Barra de progresso\n",
    "import groq #API para o Llama 3 70B\n",
    "import numpy as np #Operações com arrays\n",
    "import matplotlib.pyplot as plt #Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqInterface:\n",
    "    '''\n",
    "    Interface for using the Groq API\n",
    "\n",
    "    Implements a rate limit control for multi-threading use. \n",
    "    '''\n",
    "\n",
    "    _client :groq.Groq = None \n",
    "\n",
    "    LLAMA3_70B = \"llama3-70b-8192\"\n",
    "\n",
    "    inference_lock = threading.Lock()\n",
    "    time_waiter_lock = threading.Lock()\n",
    "    SINGLE_THREAD = True\n",
    "\n",
    "    def __init__(self, model:Optional[str]=None, api_key:Optional[str]=None, json_mode:bool=False, system_message:Optional[str]=None, n_retry:int=5):\n",
    "        '''\n",
    "        GroqInterface constructor.\n",
    "\n",
    "        Args:\n",
    "            model (str, optional): model to use. Llama3 70B is used if None. Default is None\n",
    "            api_key (str, optional): Groq API key to use, if None will check the environment 'GROQ_API_KEY' variable. Default is None.\n",
    "            json_mode (bool): if the model need to output in JSON. Default is False.\n",
    "            system_message (str): the system message to send to the model, if needed. Default is None.\n",
    "            n_retyr (int): number of times to retry if the model fails (not considering RateLimitError). Default is 5.\n",
    "        '''\n",
    "        \n",
    "        if GroqInterface._client is None:\n",
    "\n",
    "            if api_key is None:\n",
    "                api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "            if api_key is None:\n",
    "                raise RuntimeError(\"API key is not in the environment variables ('GROQ_API_KEY' variable is not set).\")\n",
    "\n",
    "            GroqInterface._client = groq.Groq(api_key=api_key)\n",
    "\n",
    "        if model is None:\n",
    "            model = GroqInterface.LLAMA3_70B\n",
    "        self._model = model\n",
    "\n",
    "        self._system_message = system_message\n",
    "\n",
    "\n",
    "        if json_mode:\n",
    "            self._response_format = {\"type\": \"json_object\"}\n",
    "        else:\n",
    "            self._response_format = None\n",
    "        self._json_mode = json_mode\n",
    "\n",
    "        self._n_retry = n_retry\n",
    "\n",
    "    def __call__(self, prompt:str) -> str:\n",
    "        '''\n",
    "        Generates the model response\n",
    "\n",
    "        Args:\n",
    "            prompt (str): prompt to send to the model.\n",
    "\n",
    "        Returns:\n",
    "            str: model response. \n",
    "        '''\n",
    "        done = False\n",
    "        retry_count = 0\n",
    "        while not done:\n",
    "            try:\n",
    "                if not GroqInterface.SINGLE_THREAD:\n",
    "                    GroqInterface.inference_lock.acquire()\n",
    "                    GroqInterface.inference_lock.release()\n",
    "\n",
    "                messages = []\n",
    "                if self._system_message is not None:\n",
    "                    messages.append({\"role\":\"system\", \"content\":self._system_message})\n",
    "                \n",
    "                messages.append({\"role\":\"user\", \"content\":prompt})\n",
    "\n",
    "                chat_completion = GroqInterface._client.chat.completions.create(\n",
    "                        messages=messages,\n",
    "                        model=self._model,\n",
    "                        response_format=self._response_format\n",
    "                    )\n",
    "                \n",
    "                done = True\n",
    "            except groq.RateLimitError as exception: #Wait\n",
    "                print(\"ERROR\")\n",
    "                print(exception)\n",
    "                \n",
    "                GroqInterface.error = exception\n",
    "                if not GroqInterface.SINGLE_THREAD:\n",
    "                    if not GroqInterface.time_waiter_lock.locked():\n",
    "                        GroqInterface.time_waiter_lock.acquire()\n",
    "                        GroqInterface.inference_lock.acquire()\n",
    "                        time.sleep(2)\n",
    "                        GroqInterface.time_waiter_lock.release()\n",
    "                        GroqInterface.inference_lock.release()\n",
    "                else:\n",
    "                    time.sleep(2)\n",
    "\n",
    "            except KeyboardInterrupt as e: #Stop the code\n",
    "                raise e\n",
    "            except Exception as e: #Retry\n",
    "                print(\"ERROR\")\n",
    "                print(e)\n",
    "                retry_count += 1\n",
    "                if retry_count >= self._n_retry:\n",
    "                    raise e\n",
    "\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tool(abc.ABC):\n",
    "    '''\n",
    "    Base class for creating LLM agent tools.\n",
    "    '''\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, query:Dict[str, str], context:str) -> Dict[str, str]:\n",
    "        '''\n",
    "        Execute the tool.\n",
    "\n",
    "        Args:\n",
    "            query (str): query for the tool execution.\n",
    "            context (str): agent context in the tool execution moment.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: tool results.\n",
    "        '''\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainTextGenerator(Tool, GroqInterface):\n",
    "\n",
    "\n",
    "    _system_message = '''You are a informative article generator that outputs in JSON. \n",
    "The JSON object must use the schema: {'article':[{'section_name':'str', 'section_text':'str'}, {'section_name':'str', 'section_text':'str'}, ...]},\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Generate a draft article for the instructions:\n",
    "\n",
    "Theme: {theme}\n",
    "Audience: {audience}\n",
    "'''\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None, api_key: Optional[str] = None):\n",
    "      \n",
    "        super().__init__(model, api_key, True, self._system_message)\n",
    "\n",
    "    def __call__(self, query:Dict[str, str], context:str=None) -> Dict[str, List[str]]:\n",
    "\n",
    "        theme = query[\"theme\"]\n",
    "        audience = query[\"audience\"]\n",
    "\n",
    "\n",
    "        prompt = self._base_prompt.format(theme=theme, audience=audience)\n",
    "\n",
    "\n",
    "        return json.loads(GroqInterface.__call__(self, prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_text_generator = MainTextGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"theme\":\"prompt engineering\", \"audience\":\"general public\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_result = main_text_generator(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': [{'section_name': 'Introduction',\n",
       "   'section_text': 'In recent years, the field of artificial intelligence (AI) has made tremendous progress, with the development of powerful language models and other AI systems that can perform tasks that were previously thought to be the exclusive domain of humans. However, as AI systems become more advanced, the need for effective communication between humans and machines has become increasingly important. This is where prompt engineering comes in, a field that focuses on designing and optimizing the inputs or prompts that people use to interact with AI systems. In this article, we will delve into the world of prompt engineering, exploring its importance, applications, and future directions.'},\n",
       "  {'section_name': 'What is Prompt Engineering?',\n",
       "   'section_text': \"At its core, prompt engineering is the process of designing and refining the inputs or prompts that people use to interact with AI systems. This can include natural language prompts, images, or other forms of input that are used to communicate with machines. The goal of prompt engineering is to optimize these inputs to get the desired response from the AI system, whether it's generating text, classifying images, or performing other tasks.\"},\n",
       "  {'section_name': 'Applications of Prompt Engineering',\n",
       "   'section_text': \"Prompt engineering has a wide range of applications across various industries. For example, in customer service, prompt engineering can be used to design optimal prompts for chatbots to provide accurate and helpful responses to customer inquiries. In healthcare, prompt engineering can be used to develop AI systems that can accurately diagnose diseases from medical images. In education, prompt engineering can be used to create personalized learning systems that adapt to individual students' needs.\"},\n",
       "  {'section_name': 'Challenges and Future Directions',\n",
       "   'section_text': 'Despite its potential, prompt engineering is still a relatively new field, and there are many challenges to be addressed. For example, designing optimal prompts that work across different AI systems and domains is a significant challenge. Additionally, as AI systems become more advanced, the need for prompt engineering will only continue to grow. As the field continues to evolve, we can expect to see new developments and innovations in prompt engineering that will shape the future of human-AI interaction.'},\n",
       "  {'section_name': 'Conclusion',\n",
       "   'section_text': 'In conclusion, prompt engineering is a critical field that has the potential to revolutionize the way humans interact with AI systems. By designing and optimizing the inputs or prompts that people use to interact with AI systems, prompt engineers can unlock the full potential of AI and improve the efficiency, accuracy, and overall user experience of AI systems. As the field continues to grow and evolve, it will be exciting to see the new developments and innovations that emerge.'}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighlightedSentencesSelector(Tool, GroqInterface):\n",
    "\n",
    "\n",
    "    _system_message = '''You are an article editor for a scientific journal. Your role at this point is to select sentences to highlight.\n",
    "The JSON object must use the schema: {'sentences': ['str', 'str', 'str']}, where sentences must be extracted from the article.\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Select sentences to highlight. Note that few, if any, sentences should be highlighted. It must be something key to the section.\n",
    "\n",
    "Section name: {name}\n",
    "Section text: {text}\n",
    "'''\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None, api_key: Optional[str] = None):\n",
    "      \n",
    "        super().__init__(model, api_key, True, self._system_message)\n",
    "\n",
    "    def __call__(self, query:Dict[str, str], context:str=None) -> Dict[str, List[str]]:\n",
    "\n",
    "        prompt = self._base_prompt.format(**query)\n",
    "\n",
    "\n",
    "        return json.loads(GroqInterface.__call__(self, prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_selector = HighlightedSentencesSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"name\":main_result[\"article\"][0][\"section_name\"], \"text\":main_result[\"article\"][0][\"section_text\"]}\n",
    "\n",
    "highlight_result0 = highlight_selector(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': ['However, as AI systems become more advanced, the need for effective communication between humans and machines has become increasingly important.',\n",
       "  'This is where prompt engineering comes in, a field that focuses on designing and optimizing the inputs or prompts that people use to interact with AI systems.']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highlight_result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefinitionSelector(Tool, GroqInterface):\n",
    "\n",
    "\n",
    "    _system_message = '''You are an article editor for a scientific journal. Your role at this point is to select terms that may be unfamiliar to the audience and create a definition to place in a text box.\n",
    "The JSON object must use the schema: {'terms': [{'term':'str', 'definition':'str'}, {'term':'str', 'definition':'str'}, ...]}, where the 'term' must be extracted from the article.\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Select terms to define. Note that few, if any, terms should be defined. It must be something key to the section and that the audience may not know.\n",
    "\n",
    "Audience: {audience}\n",
    "Section name: {name}\n",
    "Section text: {text}\n",
    "'''\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None, api_key: Optional[str] = None):\n",
    "      \n",
    "        super().__init__(model, api_key, True, self._system_message)\n",
    "\n",
    "    def __call__(self, query:Dict[str, str], context:str=None) -> Dict[str, List[str]]:\n",
    "\n",
    "        prompt = self._base_prompt.format(**query)\n",
    "\n",
    "\n",
    "        return json.loads(GroqInterface.__call__(self, prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_selector = DefinitionSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"name\":main_result[\"article\"][0][\"section_name\"], \"text\":main_result[\"article\"][0][\"section_text\"], \"audience\":\"general public\"}\n",
    "\n",
    "definition_result0 = definition_selector(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'terms': [{'term': 'prompt engineering',\n",
       "   'definition': 'The field of designing and optimizing the inputs or prompts that people use to interact with AI systems, enabling effective communication between humans and machines.'}]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definition_result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProposer(Tool, GroqInterface):\n",
    "\n",
    "\n",
    "    _system_message = '''You are an article editor for a scientific journal. Your role at this point is to propose images to the article.\n",
    "The JSON object must use the schema: {'images': [{'description':'str', 'sentence':'str'}, {'description':'str', 'sentence':'str'}, ...]}, \n",
    "where the 'description' must describe the image enough for an illustrator to create, and 'sentece' must be extracted from the article and be the sentence the image should be next to.\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Propose images. Note that few, if any, images should be defined. It must be something key to the section.\n",
    "\n",
    "Audience: {audience}\n",
    "Section name: {name}\n",
    "Section text: {text}\n",
    "'''\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None, api_key: Optional[str] = None):\n",
    "      \n",
    "        super().__init__(model, api_key, True, self._system_message)\n",
    "\n",
    "    def __call__(self, query:Dict[str, str], context:str=None) -> Dict[str, List[str]]:\n",
    "\n",
    "        prompt = self._base_prompt.format(**query)\n",
    "\n",
    "\n",
    "        return json.loads(GroqInterface.__call__(self, prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_proposer = ImageProposer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Error code: 400 - {'error': {'message': \"Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\\n\"images\": [\\n{\"description\": \"An illustration of a human interacting with a computer, with symbols and gears in the background to represent AI systems. The human and computer should be facing each other, with a speech bubble or thought bubble coming from the human, indicating the human is communicating with the AI.\", \\n\"sentence\": \"However, as AI systems become more advanced, the need for effective communication between humans and machines has become increasingly important.\"},\\n]\\n}'}}\n"
     ]
    }
   ],
   "source": [
    "image_result0 = image_proposer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': [{'description': 'An illustration showing a human interacting with a computer or robot, with speech bubbles or thought clouds representing the communication flow between the two. The human should be shown providing input to the machine, and the machine responding with output.',\n",
       "   'sentence': 'However, as AI systems become more advanced, the need for effective communication between humans and machines has become increasingly important.'}]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
