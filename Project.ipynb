{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation2Article\n",
    "\n",
    "Elton Cardoso do Nascimento - 233840\n",
    "\n",
    "IA382 - Seminar in Computer Engineering - 1s2024\n",
    "\n",
    "\n",
    "The project idea is to use a LLM to generate a informative article draft about the seminar [\"Talking to Machines: A Practical introduction to Generative AI\"](https://feec-seminar-comp-eng.github.io/seminars/seminars-1-2024/6/), by Gabriela Surita, part of 1S/2024 graduate seminar class [\"IA382\"](https://feec-seminar-comp-eng.github.io/) from Unicamp. The project input is the annotations about the seminar, the intended audience and the seminar transcription. The output is the article text draft for subsequent review and editorial treatment, divided in sections. For each section layout information is created, like sentences to highlight and terms to define in auxiliary text boxes. Images will also are proposed for each section (descriptions only, without the real image).\n",
    "\n",
    "This report, with the used and generated data, can be found in the [project Github repository](https://github.com/EltonCN/Presentation2Article). The original view of this report as a [notebook](https://github.com/EltonCN/Presentation2Article/blob/main/Project.ipynb) may be more enjoyable to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is import the libraries that will be used in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations # Type hints with |\n",
    "\n",
    "import os #Path operations\n",
    "import json #JSON read-write\n",
    "import warnings #Warnings\n",
    "from typing import Dict, Tuple #Type hints\n",
    "\n",
    "import torch #spacy don't load if not import before (??)\n",
    "import spacy #Sentence separator\n",
    "import tqdm as tqdm #Progress bar\n",
    "from pyserini.search.lucene import LuceneSearcher #Document search\n",
    "import sentence_transformers #Reranking\n",
    "import toolpy as tp #LLM Tools\n",
    "from toolpy.tool.tool import TextLike #Type hints\n",
    "from toolpy.integrations import groq #Groq interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The used LLM is the [Llama3 70B model](https://llama.meta.com/llama3/), accessed using the [Groq API](https://console.groq.com/). For creating tools with the model, [toolpy](https://github.com/EltonCN/toolpy) is used.\n",
    "\n",
    "For that, we need to set the model as the default model for the tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_interface = groq.GroqInterface(model=groq.GroqModel.LLAMA3_70B, n_retry=5)\n",
    "\n",
    "registry = tp.llm.LLMRegistry()\n",
    "registry.registry(model_name=\"llama3-70b\", interface=groq_interface, default=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is important to note that the Groq API key must be setted in the environment variable `GROQ_API_KEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "Now that the model is configured, is time for generating a outline of the article. The first tool will use the article theme and intended audience for that.\n",
    "\n",
    "A short introdution to `toolpy` tools: each tool executes a operation, receiving a query and returning the result. For a complete tool it needs to specify a description of it's operation and it's inputs, and returning the result and a description of it's parts. Several tools are based on performing simple inference with an LLM and are called \"BasicTool\". They directly define which prompts such as system and user roles will be used, with the operation being performed by the base class, and the return description must be fixed.\n",
    "\n",
    "Our first tool, \"OutlineGenerator\" works this way. Observe that, as all the other BasicTools created in this project, they expect the model to return it's results in a JSON format. The BaseClass also parses the LLM inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlineGenerator(tp.BasicTool):\n",
    "    _description = \"Creates a informative text ouline.\"\n",
    "\n",
    "    _system_message = '''You are a informative article outline generator that outputs in JSON. \n",
    "The JSON object must use the schema: {'article_name':'str', 'article':[{'section_name':'str', 'description':'str'}, {'section_name':'str', 'description':'str'}, ...]},\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Generate a draft of article sections for the instructions:\n",
    "\n",
    "Theme: {theme}\n",
    "Audience: {audience}\n",
    "'''\n",
    "\n",
    "    _return_description = {'article_name':\"name of the article\",\n",
    "        \"article\": \"list of article sections, with names and descriptions\"}\n",
    "\n",
    "    _input_description = {\"theme\":\"theme of the article\", \"audience\":\"intended audience for the article\"}\n",
    "\n",
    "    def __init__(self, model_name: str | None = None) -> None:\n",
    "        super().__init__(self._description, self._input_description, \n",
    "                         self._base_prompt, self._return_description, \n",
    "                         self._system_message, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_generator = OutlineGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BasicTools can also receive a context that is added to the user prompt, before the tool-defined prompt. We gonna use that for injecting personal keypoints annoted during the seminar, for guiding the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "context ='''\n",
    "Personal keypoints:\n",
    "- Generative AI is better defined as a negative classification: AI that is not for classificattion, regression or control.\n",
    "- Generative AI generates digital content.\n",
    "- Examples of gen AI includes sequence modeling (transformers), GAN, VAE, diffusion models.\n",
    "- The steps for making a query with a text model are: formatting, tokenization, sequence modeling (inference), detokenization and parsing.\n",
    "- Some model interfaces creates a conversation sequence with turns and roles.\n",
    "- The majority of models are instruction tuned for something, with techinics like finetuning and reinforcement learning from human feedback.\n",
    "- Instruction tuning is like a editorial decision from the model creators.\n",
    "- There is ideological discousers around generative AI, like about emergentism, escaling laws, Turing test, antropomorphism and Shannon divide.\n",
    "- We should be carefull about metaphors around generative AI, like they can \"understand\", \"think\", \"reasonate\", \"halicinate\". They can complicate or mislead the models analysis. \n",
    "- Shannon stated that semantic is irrelevant for the engineering problem of language models. Only the previous sentence words are important to describe the next word probability.\n",
    "- Shannon divide: is semantic irrelevant? Is dificult to reintroduce semantic, and we would need a \"theory of semantics\" for that.\n",
    "- Emergentism: the phenomenon is not described as the sum of parts. It can be a mirage and there is a not undestanded incremental performance improvement.\n",
    "- Escaling laws: greater the model, greater the performance. But it may not be getting better at what matter for the users (crossentropy is not language habilities).\n",
    "- Turing test: if mislead a human, it must be inteligent. But what about non-human intelligence capabilities?\n",
    "- Antropomorphism: makes us let the guard down about what this models and the companies behind they does. Is a design choice.\n",
    "- Tips for gen AI usage includes: few shot, instruction tuning (not the first choice, is expensive), don't train from the 0 (is really expensive), be aware of the bias and of the hype.\n",
    "\n",
    "The personal keypoints guide the most important parts of the seminar that will base the article.\n",
    "\n",
    "The informative article must use the above provided information. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query is defined with the theme and intended audience, as specified by the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"theme\":\"generative AI\", \"audience\":\"general public\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can the generate the article outline and inspect the generated outline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline_result, _ = outline_generator(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title - Demystifying Generative AI: Understanding the Concepts and Concerns\n",
      "\n",
      "Introduction to Generative AI: Defining generative AI as a negative classification: AI that is not for classification, regression, or control, and its ability to generate digital content.\n",
      "\n",
      "Understanding Generative AI Models: Explaining sequence modeling (transformers), GAN, VAE, diffusion models as examples of generative AI and how they work, including the steps of formatting, tokenization, sequence modeling (inference), detokenization, and parsing.\n",
      "\n",
      "The Importance of Instruction Tuning: Discussing how most models are instruction tuned for specific tasks using techniques like finetuning and reinforcement learning from human feedback, and how it's like an editorial decision from the model creators.\n",
      "\n",
      "The Ideological Discourse Around Generative AI: Exploring the debates around emergentism, escalating laws, Turing test, anthropomorphism, and Shannon divide, and the need to be cautious with metaphors that can mislead the analysis of models.\n",
      "\n",
      "The Limitations of Generative AI: Discussing the limitations of generative AI, including the difficulty of reintroducing semantics and the importance of understanding the capabilities of these models.\n",
      "\n",
      "Best Practices for Using Generative AI: Providing tips for using generative AI, including few-shot learning, instruction tuning, being aware of bias and hype, and not training from scratch.\n",
      "\n",
      "Conclusion: The Future of Prompt Engineering: Summarizing the importance of prompt engineering in generative AI and its potential applications for the general public.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Title -\", outline_result[\"article_name\"]+\"\\n\")\n",
    "\n",
    "for section in outline_result[\"article\"]:\n",
    "    print(f\"{section['section_name']}: {section['description']}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference information\n",
    "\n",
    "Before the section text generation, we gonna generate a reference information for each section. This reference information will be generated from the transcription of the seminar, using a RAG (Retrieval-Augmented Generation) system.\n",
    "\n",
    "The first step is to get the seminar transcription and format it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data\\\\Seminar transcription - Only presentation.txt\", \"r\") as file:\n",
    "    seminar_transcription = file.readlines()\n",
    "\n",
    "seminar_transcription_filtered = []\n",
    "\n",
    "for line in seminar_transcription:\n",
    "    if line == \"\" or line == \"\\n\":\n",
    "        continue\n",
    "\n",
    "    seminar_transcription_filtered.append(line)\n",
    "\n",
    "seminar_transcription = \" \".join(seminar_transcription_filtered)\n",
    "seminar_transcription = seminar_transcription.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then transform it in a set of segments, each segments with `max_length` sentences: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.34it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "#12345\n",
    "#   45678\n",
    "stride = 3#2\n",
    "max_length = 5#3\n",
    "\n",
    "def window(documents, stride, max_length):\n",
    "    treated_documents = []\n",
    "\n",
    "    for j,document in enumerate(tqdm.tqdm(documents)):\n",
    "        doc = nlp(document)\n",
    "        sentences = [sent.text.strip() for sent in doc.sents]\n",
    "        \n",
    "        for i in range(0, len(sentences), stride):\n",
    "            segment = ' '.join(sentences[i:i + max_length])\n",
    "            \n",
    "            treated_documents.append({\"contents\": segment})\n",
    "\n",
    "            if i + max_length >= len(sentences):\n",
    "                break\n",
    "\n",
    "    return treated_documents\n",
    "\n",
    "documents = [seminar_transcription]\n",
    "\n",
    "treated_documents = window(documents, stride, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, we got 119 segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(treated_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segments are exported to a JSONL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data\\iirc_indices\"):\n",
    "    !mkdir data\\iirc_indices\n",
    "\n",
    "file = open(\"data/iirc_indices/contents.jsonl\",'w')\n",
    "\n",
    "for i, doc in enumerate(treated_documents):\n",
    "    doc['id'] = i\n",
    "    if doc['contents'] != \"\":\n",
    "        file.write(json.dumps(doc)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And pyserini is used to create a index from the segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyserini.index is deprecated, please use pyserini.index.lucene.\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2024-06-25 20:30:48,590 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
      "2024-06-25 20:30:48,591 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
      "2024-06-25 20:30:48,591 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
      "2024-06-25 20:30:48,591 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/iirc_indices\n",
      "2024-06-25 20:30:48,591 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
      "2024-06-25 20:30:48,591 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
      "2024-06-25 20:30:48,591 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 1\n",
      "2024-06-25 20:30:48,591 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
      "2024-06-25 20:30:48,592 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
      "2024-06-25 20:30:48,592 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
      "2024-06-25 20:30:48,592 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
      "2024-06-25 20:30:48,592 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? false\n",
      "2024-06-25 20:30:48,592 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? false\n",
      "2024-06-25 20:30:48,592 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
      "2024-06-25 20:30:48,592 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
      "2024-06-25 20:30:48,592 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
      "2024-06-25 20:30:48,593 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
      "2024-06-25 20:30:48,593 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
      "2024-06-25 20:30:48,593 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: false\n",
      "2024-06-25 20:30:48,593 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: data/iirc_index\n",
      "2024-06-25 20:30:48,595 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
      "2024-06-25 20:30:48,603 INFO  [main] index.IndexCollection (IndexCollection.java:468) - Using DefaultEnglishAnalyzer\n",
      "2024-06-25 20:30:48,603 INFO  [main] index.IndexCollection (IndexCollection.java:469) - Stemmer: porter\n",
      "2024-06-25 20:30:48,603 INFO  [main] index.IndexCollection (IndexCollection.java:470) - Keep stopwords? false\n",
      "2024-06-25 20:30:48,603 INFO  [main] index.IndexCollection (IndexCollection.java:471) - Stopwords file: null\n",
      "2024-06-25 20:30:48,695 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 1 threads initialized.\n",
      "2024-06-25 20:30:48,695 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data\\iirc_indices\n",
      "2024-06-25 20:30:48,696 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 1 file found\n",
      "2024-06-25 20:30:48,696 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
      "2024-06-25 20:30:48,811 ERROR [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:348) - pool-2-thread-1: Unexpected Exception:\n",
      "java.lang.RuntimeException: Unrecognized token 'gs': was expecting (JSON String, Number, Array, Object or token 'null', 'true' or 'false')\n",
      " at [Source: (BufferedReader); line: 108, column: 3]\n",
      "\tat com.fasterxml.jackson.databind.MappingIterator._handleIOException(MappingIterator.java:420) ~[anserini-0.22.1-fatjar.jar:?]\n",
      "\tat com.fasterxml.jackson.databind.MappingIterator.hasNext(MappingIterator.java:190) ~[anserini-0.22.1-fatjar.jar:?]\n",
      "\tat io.anserini.collection.JsonCollection$Segment.readNext(JsonCollection.java:145) ~[anserini-0.22.1-fatjar.jar:?]\n",
      "\tat io.anserini.collection.FileSegment$1.hasNext(FileSegment.java:136) ~[anserini-0.22.1-fatjar.jar:?]\n",
      "\tat io.anserini.index.IndexCollection$LocalIndexerThread.run(IndexCollection.java:287) [anserini-0.22.1-fatjar.jar:?]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
      "\tat java.lang.Thread.run(Thread.java:830) [?:?]\n",
      "Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'gs': was expecting (JSON String, Number, Array, Object or token 'null', 'true' or 'false')\n",
      " at [Source: (BufferedReader); line: 108, column: 3]\n",
      "\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:2418) ~[anserini-0.22.1-fatjar.jar:?]\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:759) ~[anserini-0.22.1-fatjar.jar:?]\n",
      "\tat com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:3038) ~[anserini-0.22.1-fatjar.jar:?]\n",
      "\tat com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleOddValue(ReaderBasedJsonParser.java:2079) ~[anserini-0.22.1-fatjar.jar:?]\n",
      "\tat com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:805) ~[anserini-0.22.1-fatjar.jar:?]\n",
      "\tat com.fasterxml.jackson.databind.MappingIterator.hasNextValue(MappingIterator.java:246) ~[anserini-0.22.1-fatjar.jar:?]\n",
      "\tat com.fasterxml.jackson.databind.MappingIterator.hasNext(MappingIterator.java:186) ~[anserini-0.22.1-fatjar.jar:?]\n",
      "\t... 6 more\n",
      "2024-06-25 20:30:48,928 WARN  [main] index.IndexCollection (IndexCollection.java:575) - Unexpected difference between number of indexed documents and index maxDoc.\n",
      "2024-06-25 20:30:48,928 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 106 documents indexed\n",
      "2024-06-25 20:30:48,929 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
      "2024-06-25 20:30:48,929 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:                0\n",
      "2024-06-25 20:30:48,929 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
      "2024-06-25 20:30:48,929 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
      "2024-06-25 20:30:48,929 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
      "2024-06-25 20:30:48,929 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
      "2024-06-25 20:30:48,933 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 106 documents indexed in 00:00:00\n"
     ]
    }
   ],
   "source": [
    "!python -m pyserini.index -collection JsonCollection -generator DefaultLuceneDocumentGenerator -threads 1 -input data/iirc_indices -index data/iirc_index -storeRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = \"./data/iirc_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next tool, \"SearchTool\", searchs the documents for term correspondences. After that, a dense reranking is used, that is, a cossine similarity between the segments embeddings and query term embedding. The reranking filters the top-k relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchTool(tp.Tool):\n",
    "    _description = \"Searchs for documents with possible usefull information using the query.\"\n",
    "    _input_description = {\"search_term\":\"term to search\"}\n",
    "    _return_description = {\"search_result\":\"informating resulted from the search\"}\n",
    "\n",
    "\n",
    "    def __init__(self, index_path:str, embedder_model:str=\"all-MiniLM-L6-v2\", search_k:int=20, rerank_k:int=5) -> None:\n",
    "        super().__init__(self._description, self._input_description)\n",
    "\n",
    "        if search_k < rerank_k:\n",
    "            warnings.warn(f\"search_k is less than rerank_k. The result will be of search_k size. ({search_k} < {rerank_k})\")\n",
    "        \n",
    "        self._search_k = search_k\n",
    "        self._rerank_k = rerank_k\n",
    "\n",
    "        self._embedder = sentence_transformers.SentenceTransformer(embedder_model)\n",
    "        self._searcher = LuceneSearcher(index_path)\n",
    "\n",
    "    def _execute(self, query: Dict[str, str] | None, context: str | None=None) -> Tuple[Dict[str, TextLike], Dict[str, str]]:\n",
    "        #Initial search\n",
    "        search_term = query[\"search_term\"]\n",
    "        search_result = self._searcher.search(search_term, k=self._search_k)\n",
    "        \n",
    "        #Get documents\n",
    "        search_docs = []\n",
    "        for result in search_result:\n",
    "            result = json.loads(result.raw)\n",
    "\n",
    "            search_docs.append(result[\"contents\"])\n",
    "\n",
    "        #Rerank and filter\n",
    "        query_embedding = self._embedder.encode(search_term, convert_to_tensor=True)\n",
    "        search_embeddings = self._embedder.encode(search_docs, convert_to_tensor=True)\n",
    "\n",
    "        rerank_result = sentence_transformers.util.semantic_search(query_embedding, search_embeddings, top_k=self._rerank_k)\n",
    "\n",
    "        #Generate response with the selected documents\n",
    "        response = \"\"\n",
    "        for result in rerank_result[0]:\n",
    "            index = result[\"corpus_id\"]\n",
    "            result_doc = search_docs[index]\n",
    "\n",
    "            response += result_doc + \"\\n\"\n",
    "        \n",
    "        response = {\"search_result\":response}\n",
    "        return response, self._return_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = SearchTool(index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A example of the tool usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So this is the key idea no mathematics so far, but you can ask like, how do you do this? And but those familiar with the answer like for those familiar with  the field. we usually do it with a probabilistic model. I tend to think that's specially for language. This is not this is not an obvious leap like and I think it's probably one of  the most  powerful ideas of information Theory and information transmitting in general like I think Shannon made this leap of faith that if you can.\n",
      "I tend to think that's specially for language. This is not this is not an obvious leap like and I think it's probably one of  the most  powerful ideas of information Theory and information transmitting in general like I think Shannon made this leap of faith that if you can. rap sentences out of their meaning and just model them as  probabilities you can actually do something some. like quite powerful things like and basically the the key idea behind this is that you can express. the probability of the next element in a sequence for example the probability of fox given the quick Brown as You can express this as probability distributions over the next word and you can probably maximize probability of fox given to a quick brown and you can maximize probability of 13 given the beginning of the supernet.\n",
      "So be aware of entrepromorphizing and I think the final thing that I want to talk about is the shenon Divide. I think this is a term that I  invite invented but if you go back to this light on Shannon, I think like by construction We removed any notion of meaning from symbols and we built our whole system around this  property that so by construction we removed meaning so it's hard to reintroduce meaning like when you take the outputs of assistance without a theory a theory of semantics. So this doesn't say that these series  of semantics don't exist, but you need one like otherwise, it's there is a There's a flaw in your like. epistemic step by step. reasoning on how how many arises so I think the Practical conclusion here is  be very careful when you apply meaning and this usually showing metaphors to the outputs of these systems.\n",
      "You can also measure like probabilities probably the probabilities of bananas given the experiment suggests is very unlikely. but I think this idea of expressing. sequence the next element of a sequence as a probability over all possible elements vocabulary and given the conditionals of the previous elements of the sequence a very very powerful idea dear and I think like is a direct result from Shannon's theory of communication. So this is quite important. We're gonna go back to this sometime soon.\n",
      "There's also does notification like it's making a look like Disney characters. I think that's that's also phenomena that happens. Yeah. So be aware of entrepromorphizing and I think the final thing that I want to talk about is the shenon Divide. I think this is a term that I  invite invented but if you go back to this light on Shannon, I think like by construction We removed any notion of meaning from symbols and we built our whole system around this  property that so by construction we removed meaning so it's hard to reintroduce meaning like when you take the outputs of assistance without a theory a theory of semantics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = {\"search_term\":\"Shannon\"}\n",
    "result, _ = search_tool(query)\n",
    "print(result[\"search_result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what to search for? The next tools selects a list of serach terms from a topic and it's description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchTermSelector(tp.BasicTool):\n",
    "    _description = \"Selects a list of terms to search for a better topic undestanding.\"\n",
    "\n",
    "    _system_message = '''You are a search term selector that outputs in JSON. \n",
    "The JSON object must use the schema: {'search_terms':['str', 'str', ...]},\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Select terms for a better undestanding of the topic:\n",
    "\n",
    "Topic: {topic}\n",
    "Topic description: {description}\n",
    "'''\n",
    "\n",
    "    _return_description = {\"search_terms\": \"list of search terms\"}\n",
    "\n",
    "    _input_description = {\"topic\":\"topic to search form\", \"description\":\"brief topic description for directing the search\"}\n",
    "\n",
    "    def __init__(self, model_name: str | None = None) -> None:\n",
    "        super().__init__(self._description, self._input_description, \n",
    "                         self._base_prompt, self._return_description, \n",
    "                         self._system_message, model_name, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term_selector = SearchTermSelector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then generate search terms from each section, using its name as the topic and its description as the topic description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in outline_result[\"article\"]:\n",
    "    query = {\"topic\":section[\"section_name\"], \"description\":section[\"description\"]}\n",
    "\n",
    "    terms_result, _ = search_term_selector(query)\n",
    "\n",
    "    section[\"search_terms\"] = terms_result[\"search_terms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['generative ai definition',\n",
       " 'ai types',\n",
       " 'machine learning categories',\n",
       " 'digital content creation',\n",
       " 'non classification ai',\n",
       " 'ai versus ml',\n",
       " 'deep learning applications',\n",
       " 'creative ai',\n",
       " 'intelligent content generation']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outline_result[\"article\"][0][\"search_terms\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the pure search result will be too much text for the Groq API token limit, and it can also confuse the model, being a very long text with repeated information. The Summarizer tool is created for summarizing a text according to a focal topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(tp.BasicTool):\n",
    "    _description = \"Summarizes a text according to a focal topic.\"\n",
    "\n",
    "    _system_message = '''You are a text summarizer that outputs in JSON. \n",
    "The JSON object must use the schema: {'summary':'str'},\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''{text}\n",
    "\n",
    "Summarizes the above text, focusing on the following topic:\n",
    "\n",
    "Focal topic: {topic}\n",
    "'''\n",
    "\n",
    "    _return_description = {\"summary\": \"summarized text\"}\n",
    "\n",
    "    _input_description = {\"text\":\"text to summarize\", \"topic\":\"topic to focus the summary\"}\n",
    "\n",
    "    def __init__(self, model_name: str | None = None) -> None:\n",
    "        super().__init__(self._description, self._input_description, \n",
    "                         self._base_prompt, self._return_description, \n",
    "                         self._system_message, model_name, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = Summarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the section reference information can be generated, searching for the search terms in the seminar transcript, and summarizing the retrived information focusing in the search term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in outline_result[\"article\"]:\n",
    "    section[\"reference_information\"] = []\n",
    "\n",
    "    for search_term in section[\"search_terms\"]:\n",
    "        query = {\"search_term\":search_term}\n",
    "        search_result, _ = search_tool(query)\n",
    "\n",
    "        query = {\"text\":search_result[\"search_result\"], \"topic\":search_term}\n",
    "        summary_result, _ = summarizer(query)\n",
    "\n",
    "        section[\"reference_information\"].append(summary_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary': \"Generative AI refers to AI that produces unbounded digital content, often associated with creative tasks, and involves sequence modeling, which is a predictive system that can predict the next element of a sequence. It's differentiated from other types of AI, such as predictive AI, and is key to understanding systems that involve producing content.\"},\n",
       " {'summary': 'There are two main types of AI: predictive AI, associated with statistical tasks, and generative AI, associated with open-ended creative tasks and digital content creation. Generative AI includes chatbots, text-to-image systems, and other multimodal agents, and is often differentiated from narrow AI.'},\n",
       " {'summary': 'The text discusses machine learning categories, distinguishing them from classification and regression, and introduces the concept of human consumable content generation, such as text, videos, images, and audio, enabled by machines or systems. It highlights that this type of machine learning is different from traditional classification and regression, and is capable of producing unique compilations of content.'},\n",
       " {'summary': 'Generative AI is a type of AI that produces human-consumable digital content, such as text, videos, images, and audio, and is not limited to classification, regression, or multitask learning. Examples of generative AI include chatbots, text-to-image and text-to-audio systems, which can generate new content.'},\n",
       " {'summary': 'Generative AI is a type of AI that produces human-consumable content such as text, videos, images, or audio, and is often associated with open-ended creative tasks, differentiating it from predictive AI which is associated with statistical tasks. It does not involve classification, regression, or multi-choice action selection, and is exemplified by language models like GPT, Gemini, and Claude, as well as chatbots and text-to-image systems.'},\n",
       " {'summary': 'The text does not explicitly discuss the difference between AI and ML. However, it mentions AI subfields such as predictive AI, associated with statistical tasks, and generative AI, associated with open-ended creative tasks. The speaker highlights the importance of recognizing intelligence that does not look human, citing examples like AIS predicting protein structures and generating AI as transformative opportunities in the field.'},\n",
       " {'summary': 'The speaker discusses deep learning applications, specifically in text completion and chatbots, explaining how they work and how to train large sequence models. The presentation will cover hands-on tips and tricks for using these models, as well as providing an introduction to generative AI and its applications.'},\n",
       " {'summary': \"Generative AI is associated with open-ended creative tasks, distinguishes from predictive AI which is classed with statistical tasks. It enables modern AI assistants, such as chatbots and text-to-image systems, which are incredibly powerful and can help humans, but may not be recognized as intelligent because they don't look like human intelligence.\"},\n",
       " {'summary': 'The speaker is introducing the concept of generative AI, specifically sequence modeling, which is a key component in modern chatbots and other interactive systems. Sequence models are predictive systems that generate the next element in a sequence, and understanding them is crucial for developing intelligent content generation. The presentation will cover the technical basics of sequence modeling and its applications in AI, followed by a more abstract discussion on the interpretations and philosophy of AI.'}]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outline_result[\"article\"][0][\"reference_information\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that, because of the parallel sequence of operations used according to each search term, the reference information can contain repeated information between terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generator\n",
    "\n",
    "With the names, descriptions and reference information of the sections, we can generate their text.\n",
    "\n",
    "The SectionTextGenerator uses this informations for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectionTextGenerator(tp.BasicTool):\n",
    "    _description = \"Creates a informative text section.\"\n",
    "\n",
    "    _system_message = '''You are a informative article writter that outputs in JSON. \n",
    "The JSON object MUST use the schema: {'section_text':'str'},\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Write the text for the the text for the following section of the text:\n",
    "\n",
    "Section name: {section_name}\n",
    "Section description: {section_description}\n",
    "Text intended audience: {audience}\n",
    "\n",
    "Don't forget to use the JSON schema: {{'section_text':'str'}}\n",
    "'''\n",
    "\n",
    "    _return_description = {\"section_text\": \"section text\"}\n",
    "\n",
    "    _input_description = {\"section_name\":\"name of the section\", \n",
    "                          \"section_description\":\"description of the section\",\n",
    "                          \"audience\":\"text intended audience\"}\n",
    "\n",
    "    def __init__(self, model_name: str | None = None) -> None:\n",
    "        super().__init__(self._description, self._input_description, \n",
    "                         self._base_prompt, self._return_description, \n",
    "                         self._system_message, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_text_generator = SectionTextGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first tool, a context is created with the reference information for injecting this information in the tool. \n",
    "\n",
    "Also, because of the more complex and long prompt, the LLM result can sometimes not be in a valid JSON format. We repeat the query if that occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:08<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "for section in tqdm.tqdm(outline_result[\"article\"]):\n",
    "    reference_information = [information[\"summary\"] for information in section[\"reference_information\"]]\n",
    "    reference_information = \"\\n\".join(reference_information)\n",
    "\n",
    "    context = '''{reference_information}\n",
    "    \n",
    "Use the information above as a basis when writing the text for the section.'''\n",
    "\n",
    "    query = {\"section_name\":section[\"section_name\"], \n",
    "             \"section_description\":section[\"description\"],\n",
    "             \"audience\":\"general public\"}\n",
    "    \n",
    "    generator_result = None\n",
    "    while generator_result is None:\n",
    "        try:\n",
    "            generator_result, _ = section_text_generator(query)\n",
    "        except groq.groq.groq.BadRequestError:\n",
    "            pass\n",
    "\n",
    "\n",
    "    section[\"text\"] = generator_result[\"section_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is a type of artificial intelligence that doesn't fit into the traditional categories of AI, such as classification, regression, or control. Instead, its primary function is to create new, original digital content. This can include images, music, text, and even entire videos. Think of it as a creative partner that can generate ideas, complete tasks, and bring new concepts to life. With its ability to produce novel and diverse content, generative AI is revolutionizing industries and changing the way we experience digital media.\n"
     ]
    }
   ],
   "source": [
    "print(outline_result[\"article\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special elements\n",
    "\n",
    "The next step is to create special elements blocks. Three elements will be generated:\n",
    "\n",
    "- Highlights: quotes from the text to highlight next to it paragraph.\n",
    "- Definitions: terms to define.\n",
    "- Images: proposal of images for the section.\n",
    "\n",
    "They are editorial elements that will need further processing before the creation of the final article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlights\n",
    "\n",
    "For the highlights, we create a tool to select sentences from the sections to highlight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighlightedSentencesSelector(tp.BasicTool):\n",
    "    _description = \"Selects sentences to higlight from a scientific text.\"\n",
    "\n",
    "    _system_message = '''You are an article editor for a scientific journal. Your role at this point is to select sentences to highlight.\n",
    "The JSON object MUST use the schema: {'sentences': ['str', 'str', 'str', ...]}, where sentences must be extracted from the article.\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Select sentences to highlight from the text. \n",
    "Note that few, if any, sentences should be highlighted. It must be something key to the section.\n",
    "\n",
    "Text intended audience: {audience}\n",
    "Text name: {name}\n",
    "Text description: {description}\n",
    "Text : {text}\n",
    "\n",
    "Don't forget to use the JSON schema: {{'sentences': ['str', 'str', 'str', ...]}}\n",
    "'''\n",
    "\n",
    "    _return_description = {\"sentences\": \"list of sentences to highlight\"}\n",
    "\n",
    "    _input_description = {\"name\":\"name of the section\", \n",
    "                          \"description\":\"description of the section\",\n",
    "                          \"text\":\"text of the section.\",\n",
    "                          \"audience\":\"text intended audience\"}\n",
    "\n",
    "    def __init__(self, model_name: str | None = None) -> None:\n",
    "        super().__init__(self._description, self._input_description, \n",
    "                         self._base_prompt, self._return_description, \n",
    "                         self._system_message, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_selector = HighlightedSentencesSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:15<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "for section in tqdm.tqdm(outline_result[\"article\"]):\n",
    "    \n",
    "    query = {\"name\":section[\"section_name\"], \n",
    "             \"description\":section[\"description\"],\n",
    "             \"audience\":\"general public\",\n",
    "             \"text\":section[\"text\"]}\n",
    "    \n",
    "    highlight_result = None\n",
    "    while highlight_result is None:\n",
    "        try:\n",
    "            highlight_result, _ = highlight_selector(query)\n",
    "        except groq.groq.groq.BadRequestError:\n",
    "            pass\n",
    "\n",
    "\n",
    "    section[\"higlighted_sentences\"] = highlight_result[\"sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Generative AI is a type of artificial intelligence that doesn't fit into the traditional categories of AI, such as classification, regression, or control.\""
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outline_result[\"article\"][0][\"higlighted_sentences\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, the LLM can sometimes select sentences that aren't in the text, or that not appears exactly as presented. Because of that we need to filter out incorrect sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in outline_result[\"article\"]:\n",
    "    \n",
    "    to_remove = []\n",
    "    for sentence in section[\"higlighted_sentences\"]:\n",
    "        if sentence not in section[\"text\"]:\n",
    "            to_remove.append(sentence)\n",
    "\n",
    "    for sentence in to_remove:\n",
    "        section[\"higlighted_sentences\"].remove(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "The definitions are selected in a similar manner. They are selected terms to define from the text, with the definition, both created using the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefinitionSelector(tp.BasicTool):\n",
    "    _description = \"Selects terms to define in a scientific text.\"\n",
    "\n",
    "    _system_message = '''You are an article editor for a scientific journal. Your role at this point is to select terms that may be unfamiliar to the audience and create a definition to place in a text box.\n",
    "The JSON object must use the schema: {'terms': [{'term':'str', 'definition':'str'}, {'term':'str', 'definition':'str'}, ...]}, where the 'term' must be extracted from the article.\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Select terms to define from the section text. \n",
    "Note that few, if any, terms should be defined. It must be something key to the section and that the audience may not know.\n",
    "\n",
    "Text intended audience: {audience}\n",
    "Text name: {name}\n",
    "Text description: {description}\n",
    "Text : {text}\n",
    "\n",
    "Don't forget to use the specified JSON schema.\n",
    "'''\n",
    "\n",
    "    _return_description = {\"terms\": \"list of terms to define, with the term and definition\"}\n",
    "\n",
    "    _input_description = {\"name\":\"name of the section\", \n",
    "                          \"description\":\"description of the section\",\n",
    "                          \"text\":\"text of the section.\",\n",
    "                          \"audience\":\"text intended audience\"}\n",
    "\n",
    "    def __init__(self, model_name: str | None = None) -> None:\n",
    "        super().__init__(self._description, self._input_description, \n",
    "                         self._base_prompt, self._return_description, \n",
    "                         self._system_message, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_selector = DefinitionSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "for section in tqdm.tqdm(outline_result[\"article\"]):\n",
    "    \n",
    "    query = {\"name\":section[\"section_name\"], \n",
    "             \"description\":section[\"description\"],\n",
    "             \"audience\":\"general public\",\n",
    "             \"text\":section[\"text\"]}\n",
    "    \n",
    "    definition_result = None\n",
    "    while definition_result is None:\n",
    "        try:\n",
    "            definition_result, _ = definition_selector(query)\n",
    "            section[\"definitions\"] = definition_result[\"terms\"]\n",
    "            \n",
    "        except groq.groq.groq.BadRequestError:\n",
    "            definition_result = None\n",
    "            pass\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'term': 'regression',\n",
       "  'definition': 'A type of machine learning task where a model predicts a continuous or numerical value based on input data.'}]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outline_result[\"article\"][0][\"definitions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images\n",
    "\n",
    "The images are proposed for each section text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProposer(tp.BasicTool):\n",
    "    _description = \"Proposes images for a scientific text.\"\n",
    "\n",
    "    _system_message = '''You are an article editor for a scientific journal. Your role at this point is to propose images for the text.\n",
    "The JSON object must use the schema: {'images': [{'name':'str', 'description':'str'}, {'name':'str', 'description':'str'}, ...]}.\n",
    "\n",
    "Please use a valid JSON format.'''\n",
    "\n",
    "    _base_prompt = '''Propose images for the text. \n",
    "Note that few, if any, images should be proposed. It must be something key to the section and usefull for the audience.\n",
    "\n",
    "Text intended audience: {audience}\n",
    "Text name: {name}\n",
    "Text description: {description}\n",
    "Text : {text}\n",
    "\n",
    "Don't forget to use the specified JSON schema.\n",
    "'''\n",
    "\n",
    "    _return_description = {\"images\": \"list of proposed images, with name and description\"}\n",
    "\n",
    "    _input_description = {\"name\":\"name of the section\", \n",
    "                          \"description\":\"description of the section\",\n",
    "                          \"text\":\"text of the section.\",\n",
    "                          \"audience\":\"text intended audience\"}\n",
    "\n",
    "    def __init__(self, model_name: str | None = None) -> None:\n",
    "        super().__init__(self._description, self._input_description, \n",
    "                         self._base_prompt, self._return_description, \n",
    "                         self._system_message, model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_proposer = ImageProposer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for section in tqdm.tqdm(outline_result[\"article\"]):\n",
    "    \n",
    "    query = {\"name\":section[\"section_name\"], \n",
    "             \"description\":section[\"description\"],\n",
    "             \"audience\":\"general public\",\n",
    "             \"text\":section[\"text\"]}\n",
    "    \n",
    "    result = None\n",
    "    while result is None:\n",
    "        try:\n",
    "            result, _ = image_proposer(query)\n",
    "            section[\"images\"] = result[\"images\"]\n",
    "            \n",
    "        except groq.groq.groq.BadRequestError:\n",
    "            result = None\n",
    "            pass\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Generative AI Creative Partner',\n",
       "  'description': 'Illustration of a robot partner with a creative spark, surrounded by digital content'}]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outline_result[\"article\"][0][\"images\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article export\n",
    "\n",
    "With the generated informations, we than export a draft. We will use a Markdown format for that, in special the [\"Obsidian\" Markdown flavor](https://help.obsidian.md/Editing+and+formatting/Basic+formatting+syntax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, the `generate_callout` function is defined for creating callouts for the special elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_callout(text:str, type:str, name:str=\"\") -> str: \n",
    "    text = \"> \" + text.replace(\"\\n\", \"\\n> \")\n",
    "    callout = f\"> [!{type}]- {name}\\n\"+text\n",
    "    \n",
    "    return callout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We than export the final draft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generated_article.md\", \"w\") as file:\n",
    "    file.write(f\"# {outline_result['article_name']}\\n\")\n",
    "\n",
    "    abstract = \"\"\n",
    "    for section in outline_result[\"article\"]:\n",
    "        abstract += f\"1. **{section['section_name']}**: {section['description']}\\n\"\n",
    "\n",
    "    abstract = generate_callout(abstract, \"abstract\")\n",
    "    file.write(abstract+\"\\n\\n\")\n",
    "\n",
    "    for section_index, section in enumerate(outline_result[\"article\"]):\n",
    "        file.write(f\"## {section_index+1} - {section['section_name']}\\n\")\n",
    "\n",
    "        for image in section[\"images\"]:\n",
    "            image_callout = generate_callout(image[\"description\"],\n",
    "                                             \"todo\",\n",
    "                                             f\"Image - {image['name']}\")\n",
    "            \n",
    "            file.write(image_callout+\"\\n\\n\")\n",
    "\n",
    "        paragraphs = section[\"text\"].split(\"\\n\")\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            for sentence in section[\"higlighted_sentences\"]:\n",
    "                if sentence in paragraph:\n",
    "                    quotation = generate_callout(sentence, \"quote\")\n",
    "                    file.write(quotation+\"\\n\\n\")\n",
    "            \n",
    "            for definition in section[\"definitions\"]:\n",
    "                if definition[\"term\"] in paragraph:\n",
    "                    definition = generate_callout(definition[\"definition\"], \n",
    "                                                  \"info\", \n",
    "                                                  definition[\"term\"])\n",
    "                    \n",
    "                    file.write(definition+\"\\n\\n\")\n",
    "\n",
    "            file.write(paragraph+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rendered **result** can be viewed in the project repository: [generated_article.pdf](https://github.com/EltonCN/Presentation2Article/blob/main/generated_article.pdf).\n",
    "\n",
    "It contains all the generated information, and can be used as a starting point for creating the final article, with suggestions of sections, texts, images, quotes and definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Analyzing the generated draft contents, it is possible to observe how it contains information taken from the presentation, while also including some new information that was existing in the model weights, like the other model types definitions (GAN, VAE). The presented technique can be useful for generating informative materials for presentations, and perhaps even expository classes.\n",
    "\n",
    "However, a bad aspect of the generated article is the number of sections with just one paragraph. The key topics also seem to have restricted the information used from the seminar too much, which may not be desirable.\n",
    "\n",
    "Futures possible developments includes:\n",
    "- Use of information from selected articles during the discipline's bibliographic review task\n",
    "-  Export to a LaTeX file with formatting closer to the final article, facilitating the editorial process.\n",
    "-  Use of a image generative model for generating drafts of the proposed images.\n",
    "- Perform a global summary of the seminar as an alternative to the information used to generate the outline (key topics)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
